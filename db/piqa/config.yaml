abstract: 'PIQA: Reasoning about Physical Commonsense in Natural Language - To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today''s natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains - such as news articles and encyclopedia entries, where text is plentiful - in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical common-sense questions without experiencing the physical world? In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95% accuracy), large pretrained models struggle (77%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.'
arxiv: https://arxiv.org/abs/1911.11641
construction:
  class: mcq
  n_choices: 2
few_shot: 50
language: eng
license: afl-3.0
task_name: piqa
timeline: ~2019.11.26
user_prompt_template:
  example: 'Question: {centerpiece} A. {options[0]} B. {options[1]}? Answer: {correct_options[0]} \n\n'
  test: 'Question: {centerpiece} A. {options[0]} B. {options[1]}? Answer: '
version: v1
web_source:
  file:
    dev: piqa/data/train.jsonl[:101]
    test: piqa/data/valid.jsonl
  location: https://github.com/ybisk/ybisk.github.io/
