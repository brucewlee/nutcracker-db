abstract: Tasks That Language Models Don't Learn - We argue that there are certain visual-auditory properties of language that our current language modeling formulation will likely never train without an adequate multi-modal expansion. Due to the fundamental difference in how we read language and how sensory-deprived machines read language, we naturally perceive and use these visual-auditory properties, but machines do not. As our NLP community advances toward a safe Human-LLM alignment, this deficiency of knowledge can pose an unexpected yet difficult-to-solve challenge, and we make a particular connection to the philosophical case of Mary (Jackson, 1986). We present an empirical investigation through a series of tasks, termed H-TEST, designed to explore the limits of LLMs in understanding the sensory aspects of language. Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limitations of knowledge acquired in the absence of sensory experience.
arxiv: https://arxiv.org/abs/2110.14168
construction:
  class: mcq
  n_choices: 2
few_shot: 50
language: eng
license: mit
system_prompt: null
task_name: htest-hyphenated-word
timeline: ~2024.01.15
user_prompt_template:
  example: "Question: \"{centerpiece}\" Answer: {correct_options[0]} \n\n"
  test: "Question: \"{centerpiece}\" Answer: \n{options[0]}\n{options[1]} (Respond in one letter and nothing else)"
version: v1
web_source:
  file:
    dev: htest_generated_with_seed_12062023/*_dev.jsonl
    test: htest_generated_with_seed_12062023/*_test.jsonl
  location: https://github.com/brucewlee/H-Test
